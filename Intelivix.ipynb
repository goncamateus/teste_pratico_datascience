{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9n-hHKpUtPP"
   },
   "source": [
    "# Código feito por Mateus Machado para análise da Intelivix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "wIoU-CrIUtPS",
    "outputId": "5bbb580d-3d5f-4be1-eaf0-ed632397108c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aKTZeqPGUtPa"
   },
   "source": [
    "# Análise dos dados\n",
    "Vamos dar uma olhada no nosso banco de dados, se ele está balanceado ou não, se ele é consistente ou não.\n",
    "\n",
    "Transformei o .tsv em DataFrame do Pandas e vamos checar só as classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1XLThQQUtPb"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.tsv', sep='\\t', header=0)\n",
    "feels = np.array(df['Sentimento'])\n",
    "feelings = [0, 0, 0, 0, 0]\n",
    "for f in feels:\n",
    "    feelings[f] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "3QBd6pvfUtPg",
    "outputId": "01750838-54a4-4057-fcbc-903b6478a200"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=1, figsize=(9, 3))\n",
    "plt.bar(['negativo', 'um pouco negativo', 'neutro',\n",
    "         'um pouco positivo', 'positivo'], feelings)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SqM3Z6DKUtPk"
   },
   "source": [
    "Como podemos ver, o banco está longe de estar balanceado. Mas seria esse o jeito certo de se analisar? Se abrirmos o train.tsv, o banco é formado por várias sentenças e algumas frases derivadas. As frases derivadas também tem um \"sentimento\" associado. Então vou checar se, em relação as sentenças completas, se o banco está balanceado ou  não."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwGZASwAUtPl"
   },
   "outputs": [],
   "source": [
    "sentences = [0]\n",
    "ids = []\n",
    "last = len(sentences)\n",
    "for i, idsent in enumerate(df.IdSentenca):\n",
    "    if idsent > sentences[-1]:\n",
    "        sentences.append(idsent)\n",
    "        ids.append(i)\n",
    "\n",
    "feelings = [0, 0, 0, 0, 0]\n",
    "for i in ids:\n",
    "    feelings[df.iloc[i]['Sentimento']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "V04T8ux3UtPo",
    "outputId": "0fb49b60-a524-4a2c-c469-2251c836974a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vamos ver a veradeira distribuição de classes em relação às sentenças\n",
    "plt.figure(num=2, figsize=(9, 3))\n",
    "plt.bar(['negativo', 'um pouco negativo', 'neutro',\n",
    "         'um pouco positivo', 'positivo'], feelings)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kqd44cyNUtPs"
   },
   "source": [
    "Bem,  descobrimos que é um banco desbalanceado e além disso, inconsistente. Se formos ver o Id final da última sentença, ele dita 8544, porém só contamos 8530 sentenças. \n",
    "\n",
    "De primeira vista, iria balancear o banco, porém nunca trabalhei com classificação de texto. Vamos tentar sem balancear para ver o resultado e depois balanceando.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3f-QRpZswA5Z"
   },
   "source": [
    "# Preprocessamento dos dados\n",
    "\n",
    "Primeiramente, vamos \"tokenizar\" as sentenças, remover as \"stopwords\" e associar o vetor de tokens ao sentimento.\n",
    "Para isso utilizei o CountVetorizer do Scikit. Depois vamos construir nossos vetores como features, usando frequência de termos e inverso de frequência de termos no documento (TF-IDF).\n",
    "\n",
    "Após transformar as palavras do banco em features, podemos treinar nossos classificadores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwrqjU3i6DF2"
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(df.Texto)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "X_train = X_tfidf[:-1000]\n",
    "X_test = X_tfidf[-1000:]\n",
    "Y_train = df.Sentimento[:-1000]\n",
    "Y_test = df.Sentimento[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RhdW2dqrt7k"
   },
   "source": [
    "# Multinomial Naive Bayes\n",
    "O primeiro algoritmo de classificação  que escolhi foi o Multinomial Naive Bayes, o qual é clássico para classificação de texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3YILyszlUtP2",
    "outputId": "5c483584-9de9-48ca-a527-b08bfcd8a514",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wLLnxZ_NUtP6",
    "outputId": "3b6802c4-346b-41f1-f5a0-7f5ac1bfe0a8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Multinomial Naive Bayes 59.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Acurácia Multinomial Naive Bayes\",MNB.score(X_test, Y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CySVAEpUlw2v"
   },
   "source": [
    "# Linear Support Vector Classification\n",
    "\n",
    "O segundo algoritmo que escolhi foi uma Support Vector Machine (SVM) de caráter linear. Funciona de forma parecida com o classificador de regressão linear fornecido pela Sciktit porém, supostamente, tem  melhor desempenho para grande quantidade de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "aGdcyeXTnlNL",
    "outputId": "41a57cb6-2fdd-469e-ba93-a46c1b48408e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC = LinearSVC()\n",
    "SVC.fit(X_train,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dJ-jCM3epnYS",
    "outputId": "3da5aa5f-5af5-49e3-ab1e-0e8b6fae0ca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Linear Support Vector Classification 61.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Acurácia Linear Support Vector Classification\",SVC.score(X_test, Y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yg4s3sy0f6R3"
   },
   "source": [
    "# Multilayer Perceptron\n",
    "O terceiro e último algoritmo que escolhi foi o Multilayer Perceptron (MLP) com 3 camadas. Baseado na quantidade de dados, mais de 150.000, e sabendo que MLPs tem uma boa taxa de acertos em grande quantidade de dados, decidi fazer o teste. \n",
    "\n",
    "Treinei a MLP para 500 épocas para garantir a convergência, porém ela converge em menos tempo. Adotando um approach de early stop seria mais rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUrRgtrLUtP_"
   },
   "outputs": [],
   "source": [
    "MLP = Sequential()\n",
    "MLP.add(Dense(units=32, activation='relu', input_dim=X_train.shape[1]))\n",
    "MLP.add(Dropout(0.15))\n",
    "MLP.add(Dense(units=64, activation='relu'))\n",
    "MLP.add(Dropout(0.15))\n",
    "MLP.add(Dense(units=128, activation='relu'))\n",
    "MLP.add(Dropout(0.15))\n",
    "MLP.add(Dense(units=5, activation='softmax'))\n",
    "MLP.compile(loss='categorical_crossentropy', optimizer='sgd',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "APA95m6fGM8Q",
    "outputId": "73dd3112-d6d5-411b-ce41-af72bc3a377a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "155060/155060 [==============================] - 11s 71us/step - loss: 1.3119 - acc: 0.5095\n",
      "Epoch 2/500\n",
      "155060/155060 [==============================] - 10s 66us/step - loss: 1.2852 - acc: 0.5099\n",
      "Epoch 3/500\n",
      "155060/155060 [==============================] - 10s 67us/step - loss: 1.2841 - acc: 0.5099\n",
      "Epoch 4/500\n",
      "155060/155060 [==============================] - 10s 67us/step - loss: 1.2825 - acc: 0.5099\n",
      "Epoch 5/500\n",
      "155060/155060 [==============================] - 10s 68us/step - loss: 1.2804 - acc: 0.5099\n",
      "Epoch 6/500\n",
      "155060/155060 [==============================] - 10s 67us/step - loss: 1.2764 - acc: 0.5099\n",
      "Epoch 7/500\n",
      "155060/155060 [==============================] - 10s 67us/step - loss: 1.2695 - acc: 0.5099\n",
      "Epoch 8/500\n",
      "155060/155060 [==============================] - 10s 67us/step - loss: 1.2579 - acc: 0.5099\n",
      "Epoch 9/500\n",
      "155060/155060 [==============================] - 10s 67us/step - loss: 1.2427 - acc: 0.5108\n",
      "Epoch 10/500\n",
      "155060/155060 [==============================] - 10s 67us/step - loss: 1.2261 - acc: 0.5157\n",
      "Epoch 11/500\n",
      "155060/155060 [==============================] - 10s 67us/step - loss: 1.2109 - acc: 0.5221\n",
      "Epoch 12/500\n",
      "155060/155060 [==============================] - 10s 68us/step - loss: 1.1946 - acc: 0.5287\n",
      "Epoch 13/500\n",
      "155060/155060 [==============================] - 10s 66us/step - loss: 1.1762 - acc: 0.5373\n",
      "Epoch 14/500\n",
      "155060/155060 [==============================] - 10s 66us/step - loss: 1.1564 - acc: 0.5455\n",
      "Epoch 15/500\n",
      "155060/155060 [==============================] - 11s 74us/step - loss: 1.1379 - acc: 0.5531\n",
      "Epoch 16/500\n",
      "155060/155060 [==============================] - 11s 68us/step - loss: 1.1200 - acc: 0.5602\n",
      "Epoch 17/500\n",
      "155060/155060 [==============================] - 10s 67us/step - loss: 1.1034 - acc: 0.5660\n",
      "Epoch 18/500\n",
      "155060/155060 [==============================] - 11s 68us/step - loss: 1.0892 - acc: 0.5706\n",
      "Epoch 19/500\n",
      "155060/155060 [==============================] - 10s 64us/step - loss: 1.0734 - acc: 0.5755\n",
      "Epoch 20/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 1.0615 - acc: 0.5785\n",
      "Epoch 21/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 1.0491 - acc: 0.5820\n",
      "Epoch 22/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 1.0348 - acc: 0.5854\n",
      "Epoch 23/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 1.0204 - acc: 0.5888\n",
      "Epoch 24/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 1.0056 - acc: 0.5957\n",
      "Epoch 25/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.9873 - acc: 0.6030\n",
      "Epoch 26/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.9709 - acc: 0.6125\n",
      "Epoch 27/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.9486 - acc: 0.6224\n",
      "Epoch 28/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.9283 - acc: 0.6308\n",
      "Epoch 29/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.9077 - acc: 0.6378\n",
      "Epoch 30/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.8892 - acc: 0.6435\n",
      "Epoch 31/500\n",
      "155060/155060 [==============================] - 10s 64us/step - loss: 0.8694 - acc: 0.6514\n",
      "Epoch 32/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.8521 - acc: 0.6568\n",
      "Epoch 33/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.8390 - acc: 0.6611\n",
      "Epoch 34/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.8239 - acc: 0.6648\n",
      "Epoch 35/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.8109 - acc: 0.6697\n",
      "Epoch 36/500\n",
      "155060/155060 [==============================] - 10s 64us/step - loss: 0.8024 - acc: 0.6723\n",
      "Epoch 37/500\n",
      "155060/155060 [==============================] - 11s 70us/step - loss: 0.7893 - acc: 0.6768\n",
      "Epoch 38/500\n",
      "155060/155060 [==============================] - 12s 76us/step - loss: 0.7808 - acc: 0.6797\n",
      "Epoch 39/500\n",
      "155060/155060 [==============================] - 11s 73us/step - loss: 0.7722 - acc: 0.6825\n",
      "Epoch 40/500\n",
      "155060/155060 [==============================] - 11s 73us/step - loss: 0.7629 - acc: 0.6845\n",
      "Epoch 41/500\n",
      "155060/155060 [==============================] - 11s 70us/step - loss: 0.7560 - acc: 0.6879\n",
      "Epoch 42/500\n",
      "155060/155060 [==============================] - 11s 71us/step - loss: 0.7470 - acc: 0.6908\n",
      "Epoch 43/500\n",
      "155060/155060 [==============================] - 11s 74us/step - loss: 0.7392 - acc: 0.6937\n",
      "Epoch 44/500\n",
      "155060/155060 [==============================] - 10s 65us/step - loss: 0.7337 - acc: 0.6959\n",
      "Epoch 45/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.7297 - acc: 0.6975\n",
      "Epoch 46/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.7214 - acc: 0.7002\n",
      "Epoch 47/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.7142 - acc: 0.7033\n",
      "Epoch 48/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.7089 - acc: 0.7062\n",
      "Epoch 49/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.7052 - acc: 0.7073\n",
      "Epoch 50/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.6987 - acc: 0.7104\n",
      "Epoch 51/500\n",
      "155060/155060 [==============================] - 10s 63us/step - loss: 0.6958 - acc: 0.7114\n",
      "Epoch 52/500\n",
      "155060/155060 [==============================] - 10s 64us/step - loss: 0.6901 - acc: 0.7137\n",
      "Epoch 53/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.6826 - acc: 0.7159\n",
      "Epoch 54/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.6785 - acc: 0.7178\n",
      "Epoch 55/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.6738 - acc: 0.7198\n",
      "Epoch 56/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.6668 - acc: 0.7228\n",
      "Epoch 57/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.6659 - acc: 0.7227\n",
      "Epoch 58/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.6614 - acc: 0.7247\n",
      "Epoch 59/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.6550 - acc: 0.7269\n",
      "Epoch 60/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.6522 - acc: 0.7283\n",
      "Epoch 61/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.6474 - acc: 0.7299\n",
      "Epoch 62/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.6469 - acc: 0.7305\n",
      "Epoch 63/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.6392 - acc: 0.7337\n",
      "Epoch 64/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.6354 - acc: 0.7351\n",
      "Epoch 65/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.6340 - acc: 0.7371\n",
      "Epoch 66/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.6314 - acc: 0.7363\n",
      "Epoch 67/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.6259 - acc: 0.7387\n",
      "Epoch 68/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.6228 - acc: 0.7401\n",
      "Epoch 69/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.6214 - acc: 0.7398\n",
      "Epoch 70/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.6173 - acc: 0.7423\n",
      "Epoch 71/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.6120 - acc: 0.7444\n",
      "Epoch 72/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.6105 - acc: 0.7460\n",
      "Epoch 73/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.6081 - acc: 0.7448\n",
      "Epoch 74/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.6060 - acc: 0.7455\n",
      "Epoch 75/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.6022 - acc: 0.7481\n",
      "Epoch 76/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.5982 - acc: 0.7487\n",
      "Epoch 77/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.5992 - acc: 0.7493\n",
      "Epoch 78/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.5929 - acc: 0.7525\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.5922 - acc: 0.7521\n",
      "Epoch 80/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.5874 - acc: 0.7542\n",
      "Epoch 81/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5849 - acc: 0.7539\n",
      "Epoch 82/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5840 - acc: 0.7562\n",
      "Epoch 83/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5775 - acc: 0.7588\n",
      "Epoch 84/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5794 - acc: 0.7589\n",
      "Epoch 85/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5759 - acc: 0.7577\n",
      "Epoch 86/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5741 - acc: 0.7598\n",
      "Epoch 87/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5719 - acc: 0.7600\n",
      "Epoch 88/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5655 - acc: 0.7633\n",
      "Epoch 89/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5638 - acc: 0.7649\n",
      "Epoch 90/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5622 - acc: 0.7649\n",
      "Epoch 91/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5592 - acc: 0.7656\n",
      "Epoch 92/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5558 - acc: 0.7679\n",
      "Epoch 93/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5556 - acc: 0.7675\n",
      "Epoch 94/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5550 - acc: 0.7677\n",
      "Epoch 95/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5538 - acc: 0.7669\n",
      "Epoch 96/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5468 - acc: 0.7714\n",
      "Epoch 97/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5475 - acc: 0.7702\n",
      "Epoch 98/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5424 - acc: 0.7735\n",
      "Epoch 99/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5424 - acc: 0.7724\n",
      "Epoch 100/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5386 - acc: 0.7739\n",
      "Epoch 101/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5360 - acc: 0.7753\n",
      "Epoch 102/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5364 - acc: 0.7755\n",
      "Epoch 103/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5332 - acc: 0.7760\n",
      "Epoch 104/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5315 - acc: 0.7786\n",
      "Epoch 105/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5301 - acc: 0.7767\n",
      "Epoch 106/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5283 - acc: 0.7784\n",
      "Epoch 107/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5269 - acc: 0.7786\n",
      "Epoch 108/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5255 - acc: 0.7790\n",
      "Epoch 109/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5254 - acc: 0.7789\n",
      "Epoch 110/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5223 - acc: 0.7809\n",
      "Epoch 111/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5205 - acc: 0.7807\n",
      "Epoch 112/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5182 - acc: 0.7816\n",
      "Epoch 113/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.5171 - acc: 0.7834\n",
      "Epoch 114/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.5152 - acc: 0.7839\n",
      "Epoch 115/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.5106 - acc: 0.7857\n",
      "Epoch 116/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.5103 - acc: 0.7859\n",
      "Epoch 117/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.5078 - acc: 0.7880\n",
      "Epoch 118/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.5101 - acc: 0.7863\n",
      "Epoch 119/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5055 - acc: 0.7881\n",
      "Epoch 120/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.5054 - acc: 0.7870\n",
      "Epoch 121/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.5017 - acc: 0.7903\n",
      "Epoch 122/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.5037 - acc: 0.7884\n",
      "Epoch 123/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.5019 - acc: 0.7897\n",
      "Epoch 124/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4996 - acc: 0.7903\n",
      "Epoch 125/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4964 - acc: 0.7921\n",
      "Epoch 126/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4953 - acc: 0.7928\n",
      "Epoch 127/500\n",
      "155060/155060 [==============================] - 11s 74us/step - loss: 0.4931 - acc: 0.7926\n",
      "Epoch 128/500\n",
      "155060/155060 [==============================] - 11s 70us/step - loss: 0.4908 - acc: 0.7940\n",
      "Epoch 129/500\n",
      "155060/155060 [==============================] - 11s 72us/step - loss: 0.4922 - acc: 0.7935\n",
      "Epoch 130/500\n",
      "155060/155060 [==============================] - 11s 73us/step - loss: 0.4877 - acc: 0.7967\n",
      "Epoch 131/500\n",
      "155060/155060 [==============================] - 12s 76us/step - loss: 0.4873 - acc: 0.7947\n",
      "Epoch 132/500\n",
      "155060/155060 [==============================] - 12s 78us/step - loss: 0.4886 - acc: 0.7955\n",
      "Epoch 133/500\n",
      "155060/155060 [==============================] - 11s 72us/step - loss: 0.4861 - acc: 0.7963\n",
      "Epoch 134/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4850 - acc: 0.7961\n",
      "Epoch 135/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4847 - acc: 0.7963\n",
      "Epoch 136/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4808 - acc: 0.7989\n",
      "Epoch 137/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4795 - acc: 0.7994\n",
      "Epoch 138/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4778 - acc: 0.8008\n",
      "Epoch 139/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4771 - acc: 0.7996\n",
      "Epoch 140/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.4753 - acc: 0.8007\n",
      "Epoch 141/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4736 - acc: 0.8015\n",
      "Epoch 142/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4703 - acc: 0.8027\n",
      "Epoch 143/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4703 - acc: 0.8031\n",
      "Epoch 144/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4705 - acc: 0.8034\n",
      "Epoch 145/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.4699 - acc: 0.8036\n",
      "Epoch 146/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4662 - acc: 0.8048\n",
      "Epoch 147/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4661 - acc: 0.8060\n",
      "Epoch 148/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4665 - acc: 0.8033\n",
      "Epoch 149/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4646 - acc: 0.8064\n",
      "Epoch 150/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4627 - acc: 0.8067\n",
      "Epoch 151/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4603 - acc: 0.8074\n",
      "Epoch 152/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4612 - acc: 0.8070\n",
      "Epoch 153/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4593 - acc: 0.8076\n",
      "Epoch 154/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4581 - acc: 0.8074\n",
      "Epoch 155/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4566 - acc: 0.8076\n",
      "Epoch 156/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4573 - acc: 0.8079\n",
      "Epoch 157/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4550 - acc: 0.8101\n",
      "Epoch 158/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4543 - acc: 0.8099\n",
      "Epoch 159/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4510 - acc: 0.8109\n",
      "Epoch 160/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4515 - acc: 0.8116\n",
      "Epoch 161/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4493 - acc: 0.8123\n",
      "Epoch 162/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4468 - acc: 0.8136\n",
      "Epoch 163/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4465 - acc: 0.8127\n",
      "Epoch 164/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4489 - acc: 0.8123\n",
      "Epoch 165/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4472 - acc: 0.8144\n",
      "Epoch 166/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4441 - acc: 0.8146\n",
      "Epoch 167/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4434 - acc: 0.8133\n",
      "Epoch 168/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4415 - acc: 0.8154\n",
      "Epoch 169/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4413 - acc: 0.8154\n",
      "Epoch 170/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4413 - acc: 0.8158\n",
      "Epoch 171/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4407 - acc: 0.8153\n",
      "Epoch 172/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4393 - acc: 0.8151\n",
      "Epoch 173/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4401 - acc: 0.8152\n",
      "Epoch 174/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4364 - acc: 0.8170\n",
      "Epoch 175/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4366 - acc: 0.8174\n",
      "Epoch 176/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4352 - acc: 0.8182\n",
      "Epoch 177/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4332 - acc: 0.8191\n",
      "Epoch 178/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4342 - acc: 0.8187\n",
      "Epoch 179/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4321 - acc: 0.8189\n",
      "Epoch 180/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4315 - acc: 0.8195\n",
      "Epoch 181/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4306 - acc: 0.8200\n",
      "Epoch 182/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4293 - acc: 0.8204\n",
      "Epoch 183/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4288 - acc: 0.8209\n",
      "Epoch 184/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4282 - acc: 0.8203\n",
      "Epoch 185/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4261 - acc: 0.8226\n",
      "Epoch 186/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4252 - acc: 0.8221\n",
      "Epoch 187/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4250 - acc: 0.8222\n",
      "Epoch 188/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4238 - acc: 0.8239\n",
      "Epoch 189/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4211 - acc: 0.8241\n",
      "Epoch 190/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4244 - acc: 0.8229\n",
      "Epoch 191/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4214 - acc: 0.8235\n",
      "Epoch 192/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4231 - acc: 0.8237\n",
      "Epoch 193/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4214 - acc: 0.8237\n",
      "Epoch 194/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4198 - acc: 0.8247\n",
      "Epoch 195/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4182 - acc: 0.8256\n",
      "Epoch 196/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4162 - acc: 0.8270\n",
      "Epoch 197/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4160 - acc: 0.8260\n",
      "Epoch 198/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.4148 - acc: 0.8262\n",
      "Epoch 199/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4135 - acc: 0.8283\n",
      "Epoch 200/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4138 - acc: 0.8269\n",
      "Epoch 201/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4128 - acc: 0.8271\n",
      "Epoch 202/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4134 - acc: 0.8277\n",
      "Epoch 203/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4093 - acc: 0.8288\n",
      "Epoch 204/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4131 - acc: 0.8269\n",
      "Epoch 205/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4078 - acc: 0.8300\n",
      "Epoch 206/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4104 - acc: 0.8291\n",
      "Epoch 207/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4100 - acc: 0.8285\n",
      "Epoch 208/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.4074 - acc: 0.8296\n",
      "Epoch 209/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.4068 - acc: 0.8306\n",
      "Epoch 210/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.4066 - acc: 0.8304\n",
      "Epoch 211/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4043 - acc: 0.8303\n",
      "Epoch 212/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.4024 - acc: 0.8312\n",
      "Epoch 213/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.4032 - acc: 0.8319\n",
      "Epoch 214/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4022 - acc: 0.8322\n",
      "Epoch 215/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4029 - acc: 0.8328\n",
      "Epoch 216/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4033 - acc: 0.8321\n",
      "Epoch 217/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4019 - acc: 0.8316\n",
      "Epoch 218/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.4009 - acc: 0.8322\n",
      "Epoch 219/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.4001 - acc: 0.8335\n",
      "Epoch 220/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3992 - acc: 0.8335\n",
      "Epoch 221/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3997 - acc: 0.8335\n",
      "Epoch 222/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3978 - acc: 0.8336\n",
      "Epoch 223/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3974 - acc: 0.8337\n",
      "Epoch 224/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3963 - acc: 0.8347\n",
      "Epoch 225/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3954 - acc: 0.8350\n",
      "Epoch 226/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3954 - acc: 0.8349\n",
      "Epoch 227/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3949 - acc: 0.8354\n",
      "Epoch 228/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3924 - acc: 0.8372\n",
      "Epoch 229/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3920 - acc: 0.8370\n",
      "Epoch 230/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3953 - acc: 0.8346\n",
      "Epoch 231/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3941 - acc: 0.8361\n",
      "Epoch 232/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3914 - acc: 0.8369\n",
      "Epoch 233/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3893 - acc: 0.8377\n",
      "Epoch 234/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3913 - acc: 0.8366\n",
      "Epoch 235/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3876 - acc: 0.8384\n",
      "Epoch 236/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3869 - acc: 0.8385\n",
      "Epoch 237/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3861 - acc: 0.8394\n",
      "Epoch 238/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3881 - acc: 0.8390\n",
      "Epoch 239/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3879 - acc: 0.8379\n",
      "Epoch 240/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3858 - acc: 0.8395\n",
      "Epoch 241/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3874 - acc: 0.8392\n",
      "Epoch 242/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3838 - acc: 0.8404\n",
      "Epoch 243/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3824 - acc: 0.8398\n",
      "Epoch 244/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3852 - acc: 0.8393\n",
      "Epoch 245/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3822 - acc: 0.8411\n",
      "Epoch 246/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3823 - acc: 0.8403\n",
      "Epoch 247/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3809 - acc: 0.8415\n",
      "Epoch 248/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3824 - acc: 0.8404\n",
      "Epoch 249/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3810 - acc: 0.8410\n",
      "Epoch 250/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3808 - acc: 0.8414\n",
      "Epoch 251/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3788 - acc: 0.8422\n",
      "Epoch 252/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3796 - acc: 0.8411\n",
      "Epoch 253/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3801 - acc: 0.8418\n",
      "Epoch 254/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3799 - acc: 0.8418\n",
      "Epoch 255/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3777 - acc: 0.8415\n",
      "Epoch 256/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3781 - acc: 0.8433\n",
      "Epoch 257/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3760 - acc: 0.8429\n",
      "Epoch 258/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3768 - acc: 0.8423\n",
      "Epoch 259/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3771 - acc: 0.8433\n",
      "Epoch 260/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3733 - acc: 0.8446\n",
      "Epoch 261/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3739 - acc: 0.8442\n",
      "Epoch 262/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3740 - acc: 0.8443\n",
      "Epoch 263/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3723 - acc: 0.8453\n",
      "Epoch 264/500\n",
      "155060/155060 [==============================] - 11s 72us/step - loss: 0.3740 - acc: 0.8445\n",
      "Epoch 265/500\n",
      "155060/155060 [==============================] - 11s 73us/step - loss: 0.3697 - acc: 0.8461\n",
      "Epoch 266/500\n",
      "155060/155060 [==============================] - 11s 71us/step - loss: 0.3729 - acc: 0.8434\n",
      "Epoch 267/500\n",
      "155060/155060 [==============================] - 11s 70us/step - loss: 0.3715 - acc: 0.8464\n",
      "Epoch 268/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3721 - acc: 0.8445\n",
      "Epoch 269/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3689 - acc: 0.8464\n",
      "Epoch 270/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3709 - acc: 0.8465\n",
      "Epoch 271/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3691 - acc: 0.8460\n",
      "Epoch 272/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3693 - acc: 0.8458\n",
      "Epoch 273/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3671 - acc: 0.8473\n",
      "Epoch 274/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3685 - acc: 0.8474\n",
      "Epoch 275/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3684 - acc: 0.8467\n",
      "Epoch 276/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3657 - acc: 0.8477\n",
      "Epoch 277/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3663 - acc: 0.8482\n",
      "Epoch 278/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3663 - acc: 0.8478\n",
      "Epoch 279/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3660 - acc: 0.8474\n",
      "Epoch 280/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3628 - acc: 0.8485\n",
      "Epoch 281/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3650 - acc: 0.8481\n",
      "Epoch 282/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3634 - acc: 0.8492\n",
      "Epoch 283/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3653 - acc: 0.8481\n",
      "Epoch 284/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3648 - acc: 0.8479\n",
      "Epoch 285/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3619 - acc: 0.8496\n",
      "Epoch 286/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3637 - acc: 0.8479\n",
      "Epoch 287/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3612 - acc: 0.8507\n",
      "Epoch 288/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3608 - acc: 0.8497\n",
      "Epoch 289/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3614 - acc: 0.8494\n",
      "Epoch 290/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3599 - acc: 0.8503\n",
      "Epoch 291/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3582 - acc: 0.8496\n",
      "Epoch 292/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3581 - acc: 0.8505\n",
      "Epoch 293/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3599 - acc: 0.8497\n",
      "Epoch 294/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3588 - acc: 0.8502\n",
      "Epoch 295/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3578 - acc: 0.8499\n",
      "Epoch 296/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3564 - acc: 0.8509\n",
      "Epoch 297/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3569 - acc: 0.8511\n",
      "Epoch 298/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3569 - acc: 0.8510\n",
      "Epoch 299/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3564 - acc: 0.8515\n",
      "Epoch 300/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3556 - acc: 0.8524\n",
      "Epoch 301/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3566 - acc: 0.8524\n",
      "Epoch 302/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3543 - acc: 0.8511\n",
      "Epoch 303/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3543 - acc: 0.8519\n",
      "Epoch 304/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3535 - acc: 0.8527\n",
      "Epoch 305/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3547 - acc: 0.8521\n",
      "Epoch 306/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3527 - acc: 0.8527\n",
      "Epoch 307/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3534 - acc: 0.8524\n",
      "Epoch 308/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3527 - acc: 0.8515\n",
      "Epoch 309/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3534 - acc: 0.8531\n",
      "Epoch 310/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3524 - acc: 0.8531\n",
      "Epoch 311/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3505 - acc: 0.8534\n",
      "Epoch 312/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3509 - acc: 0.8538\n",
      "Epoch 313/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3506 - acc: 0.8536\n",
      "Epoch 314/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3511 - acc: 0.8533\n",
      "Epoch 315/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3485 - acc: 0.8538\n",
      "Epoch 316/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3486 - acc: 0.8544\n",
      "Epoch 317/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3487 - acc: 0.8545\n",
      "Epoch 318/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3489 - acc: 0.8545\n",
      "Epoch 319/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3483 - acc: 0.8546\n",
      "Epoch 320/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3485 - acc: 0.8548\n",
      "Epoch 321/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3492 - acc: 0.8533\n",
      "Epoch 322/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3479 - acc: 0.8541\n",
      "Epoch 323/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3466 - acc: 0.8546\n",
      "Epoch 324/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3470 - acc: 0.8558\n",
      "Epoch 325/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3452 - acc: 0.8554\n",
      "Epoch 326/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3470 - acc: 0.8541\n",
      "Epoch 327/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3442 - acc: 0.8561\n",
      "Epoch 328/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3449 - acc: 0.8558\n",
      "Epoch 329/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3453 - acc: 0.8557\n",
      "Epoch 330/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3438 - acc: 0.8562\n",
      "Epoch 331/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3436 - acc: 0.8566\n",
      "Epoch 332/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3419 - acc: 0.8572\n",
      "Epoch 333/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3439 - acc: 0.8565\n",
      "Epoch 334/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3418 - acc: 0.8572\n",
      "Epoch 335/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3422 - acc: 0.8569\n",
      "Epoch 336/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3420 - acc: 0.8579\n",
      "Epoch 337/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3412 - acc: 0.8576\n",
      "Epoch 338/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3433 - acc: 0.8558\n",
      "Epoch 339/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3427 - acc: 0.8564\n",
      "Epoch 340/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3411 - acc: 0.8573\n",
      "Epoch 341/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3423 - acc: 0.8563\n",
      "Epoch 342/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3399 - acc: 0.8577\n",
      "Epoch 343/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3404 - acc: 0.8574\n",
      "Epoch 344/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3396 - acc: 0.8582\n",
      "Epoch 345/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3371 - acc: 0.8586\n",
      "Epoch 346/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3383 - acc: 0.8581\n",
      "Epoch 347/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3378 - acc: 0.8587\n",
      "Epoch 348/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3403 - acc: 0.8570\n",
      "Epoch 349/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3375 - acc: 0.8584\n",
      "Epoch 350/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3387 - acc: 0.8577\n",
      "Epoch 351/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3392 - acc: 0.8586\n",
      "Epoch 352/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3385 - acc: 0.8586\n",
      "Epoch 353/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3350 - acc: 0.8600\n",
      "Epoch 354/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3379 - acc: 0.8586\n",
      "Epoch 355/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3358 - acc: 0.8600\n",
      "Epoch 356/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3358 - acc: 0.8599\n",
      "Epoch 357/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3351 - acc: 0.8596\n",
      "Epoch 358/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3339 - acc: 0.8600\n",
      "Epoch 359/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3343 - acc: 0.8610\n",
      "Epoch 360/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3362 - acc: 0.8584\n",
      "Epoch 361/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3330 - acc: 0.8609\n",
      "Epoch 362/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3342 - acc: 0.8599\n",
      "Epoch 363/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3330 - acc: 0.8614\n",
      "Epoch 364/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3336 - acc: 0.8613\n",
      "Epoch 365/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3345 - acc: 0.8603\n",
      "Epoch 366/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3312 - acc: 0.8621\n",
      "Epoch 367/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3320 - acc: 0.8614\n",
      "Epoch 368/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3317 - acc: 0.8607\n",
      "Epoch 369/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3310 - acc: 0.8616\n",
      "Epoch 370/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3300 - acc: 0.8628\n",
      "Epoch 371/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3319 - acc: 0.8604\n",
      "Epoch 372/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3313 - acc: 0.8614\n",
      "Epoch 373/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3321 - acc: 0.8608\n",
      "Epoch 374/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3300 - acc: 0.8619\n",
      "Epoch 375/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3307 - acc: 0.8619\n",
      "Epoch 376/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3301 - acc: 0.8623\n",
      "Epoch 377/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3295 - acc: 0.8621\n",
      "Epoch 378/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3286 - acc: 0.8629\n",
      "Epoch 379/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3288 - acc: 0.8628\n",
      "Epoch 380/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3285 - acc: 0.8627\n",
      "Epoch 381/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3271 - acc: 0.8633\n",
      "Epoch 382/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3305 - acc: 0.8619\n",
      "Epoch 383/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3252 - acc: 0.8635\n",
      "Epoch 384/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3273 - acc: 0.8641\n",
      "Epoch 385/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3265 - acc: 0.8629\n",
      "Epoch 386/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3259 - acc: 0.8638\n",
      "Epoch 387/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3242 - acc: 0.8649\n",
      "Epoch 388/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3245 - acc: 0.8641\n",
      "Epoch 389/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3264 - acc: 0.8624\n",
      "Epoch 390/500\n",
      "155060/155060 [==============================] - 11s 73us/step - loss: 0.3257 - acc: 0.8640\n",
      "Epoch 391/500\n",
      "155060/155060 [==============================] - 11s 71us/step - loss: 0.3249 - acc: 0.8637\n",
      "Epoch 392/500\n",
      "155060/155060 [==============================] - 11s 69us/step - loss: 0.3252 - acc: 0.8638\n",
      "Epoch 393/500\n",
      "155060/155060 [==============================] - 11s 72us/step - loss: 0.3254 - acc: 0.8633\n",
      "Epoch 394/500\n",
      "155060/155060 [==============================] - 11s 71us/step - loss: 0.3235 - acc: 0.8651\n",
      "Epoch 395/500\n",
      "155060/155060 [==============================] - 11s 70us/step - loss: 0.3259 - acc: 0.8630\n",
      "Epoch 396/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3236 - acc: 0.8644\n",
      "Epoch 397/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3239 - acc: 0.8649\n",
      "Epoch 398/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3228 - acc: 0.8652\n",
      "Epoch 399/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3242 - acc: 0.8645\n",
      "Epoch 400/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3232 - acc: 0.8645\n",
      "Epoch 401/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3237 - acc: 0.8644\n",
      "Epoch 402/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3233 - acc: 0.8645\n",
      "Epoch 403/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3247 - acc: 0.8632\n",
      "Epoch 404/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3225 - acc: 0.8644\n",
      "Epoch 405/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3230 - acc: 0.8645\n",
      "Epoch 406/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3231 - acc: 0.8645\n",
      "Epoch 407/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3204 - acc: 0.8655\n",
      "Epoch 408/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3201 - acc: 0.8645\n",
      "Epoch 409/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3218 - acc: 0.8646\n",
      "Epoch 410/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3207 - acc: 0.8654\n",
      "Epoch 411/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3209 - acc: 0.8653\n",
      "Epoch 412/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3197 - acc: 0.8650\n",
      "Epoch 413/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3194 - acc: 0.8667\n",
      "Epoch 414/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3195 - acc: 0.8665\n",
      "Epoch 415/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3178 - acc: 0.8673\n",
      "Epoch 416/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3188 - acc: 0.8666\n",
      "Epoch 417/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3192 - acc: 0.8652\n",
      "Epoch 418/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3197 - acc: 0.8657\n",
      "Epoch 419/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3176 - acc: 0.8663\n",
      "Epoch 420/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3171 - acc: 0.8667\n",
      "Epoch 421/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3194 - acc: 0.8660\n",
      "Epoch 422/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3182 - acc: 0.8670\n",
      "Epoch 423/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3168 - acc: 0.8673\n",
      "Epoch 424/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3167 - acc: 0.8674\n",
      "Epoch 425/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3180 - acc: 0.8656\n",
      "Epoch 426/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3134 - acc: 0.8677\n",
      "Epoch 427/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3158 - acc: 0.8669\n",
      "Epoch 428/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3168 - acc: 0.8666\n",
      "Epoch 429/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3175 - acc: 0.8667\n",
      "Epoch 430/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3164 - acc: 0.8671\n",
      "Epoch 431/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3180 - acc: 0.8662\n",
      "Epoch 432/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3142 - acc: 0.8678\n",
      "Epoch 433/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3165 - acc: 0.8677\n",
      "Epoch 434/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3164 - acc: 0.8674\n",
      "Epoch 435/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3151 - acc: 0.8686\n",
      "Epoch 436/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3131 - acc: 0.8689\n",
      "Epoch 437/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3147 - acc: 0.8669\n",
      "Epoch 438/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3136 - acc: 0.8673\n",
      "Epoch 439/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3132 - acc: 0.8690\n",
      "Epoch 440/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3138 - acc: 0.8683\n",
      "Epoch 441/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3117 - acc: 0.8690\n",
      "Epoch 442/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3141 - acc: 0.8676\n",
      "Epoch 443/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3129 - acc: 0.8683\n",
      "Epoch 444/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3116 - acc: 0.8693\n",
      "Epoch 445/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3130 - acc: 0.8688\n",
      "Epoch 446/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3116 - acc: 0.8688\n",
      "Epoch 447/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3125 - acc: 0.8680\n",
      "Epoch 448/500\n",
      "155060/155060 [==============================] - 10s 61us/step - loss: 0.3114 - acc: 0.8691\n",
      "Epoch 449/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3104 - acc: 0.8694\n",
      "Epoch 450/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3110 - acc: 0.8696\n",
      "Epoch 451/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3124 - acc: 0.8680\n",
      "Epoch 452/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3096 - acc: 0.8696\n",
      "Epoch 453/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3118 - acc: 0.8686\n",
      "Epoch 454/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3097 - acc: 0.8703\n",
      "Epoch 455/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3113 - acc: 0.8686\n",
      "Epoch 456/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3104 - acc: 0.8685\n",
      "Epoch 457/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3101 - acc: 0.8687\n",
      "Epoch 458/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3103 - acc: 0.8691\n",
      "Epoch 459/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3101 - acc: 0.8704\n",
      "Epoch 460/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3105 - acc: 0.8692\n",
      "Epoch 461/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3099 - acc: 0.8695\n",
      "Epoch 462/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3071 - acc: 0.8706\n",
      "Epoch 463/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3103 - acc: 0.8686\n",
      "Epoch 464/500\n",
      "155060/155060 [==============================] - 10s 62us/step - loss: 0.3099 - acc: 0.8697\n",
      "Epoch 465/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3064 - acc: 0.8698\n",
      "Epoch 466/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3082 - acc: 0.8704\n",
      "Epoch 467/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3075 - acc: 0.8700\n",
      "Epoch 468/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3079 - acc: 0.8697\n",
      "Epoch 469/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3082 - acc: 0.8694\n",
      "Epoch 470/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3078 - acc: 0.8697\n",
      "Epoch 471/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3066 - acc: 0.8709\n",
      "Epoch 472/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3081 - acc: 0.8700\n",
      "Epoch 473/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3084 - acc: 0.8708\n",
      "Epoch 474/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3062 - acc: 0.8704\n",
      "Epoch 475/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3070 - acc: 0.8712\n",
      "Epoch 476/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3068 - acc: 0.8705\n",
      "Epoch 477/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3073 - acc: 0.8711\n",
      "Epoch 478/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3067 - acc: 0.8706\n",
      "Epoch 479/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3045 - acc: 0.8713\n",
      "Epoch 480/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3049 - acc: 0.8713\n",
      "Epoch 481/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3048 - acc: 0.8721\n",
      "Epoch 482/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3061 - acc: 0.8709\n",
      "Epoch 483/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3054 - acc: 0.8714\n",
      "Epoch 484/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3057 - acc: 0.8710\n",
      "Epoch 485/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3044 - acc: 0.8727\n",
      "Epoch 486/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3046 - acc: 0.8705\n",
      "Epoch 487/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3054 - acc: 0.8714\n",
      "Epoch 488/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3042 - acc: 0.8718\n",
      "Epoch 489/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3040 - acc: 0.8720\n",
      "Epoch 490/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3019 - acc: 0.8729\n",
      "Epoch 491/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3029 - acc: 0.8715\n",
      "Epoch 492/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3034 - acc: 0.8720\n",
      "Epoch 493/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3027 - acc: 0.8724\n",
      "Epoch 494/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3029 - acc: 0.8724\n",
      "Epoch 495/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3034 - acc: 0.8719\n",
      "Epoch 496/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3030 - acc: 0.8727\n",
      "Epoch 497/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3038 - acc: 0.8726\n",
      "Epoch 498/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3045 - acc: 0.8713\n",
      "Epoch 499/500\n",
      "155060/155060 [==============================] - 9s 60us/step - loss: 0.3019 - acc: 0.8728\n",
      "Epoch 500/500\n",
      "155060/155060 [==============================] - 9s 61us/step - loss: 0.3016 - acc: 0.8730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2cd6bfdd8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = []\n",
    "for label in df.Sentimento[:-1000]:\n",
    "    l = [0 for _ in range(5)]\n",
    "    l[label] = 1\n",
    "    Y.append(l)\n",
    "MLP.fit(X_train, np.array(Y), epochs=500, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fglYB_8oUtQH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 139us/step\n",
      "Acurácia Multilayer Perceptron 58.29999980926514\n"
     ]
    }
   ],
   "source": [
    "y_test = []\n",
    "for label in Y_test:\n",
    "    l = [0 for _ in range(5)]\n",
    "    l[label] = 1\n",
    "    y_test.append(l)\n",
    "loss_and_metrics = MLP.evaluate(X_test, np.array(y_test), batch_size=128)\n",
    "print(\"Acurácia Multilayer Perceptron\",loss_and_metrics[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UH8UDG93fygE"
   },
   "source": [
    "# Balanceando os dados\n",
    "\n",
    "Tivemos resultados razoáveis com os dados desbalanceados, agora vamos fazer um experimento balanceando os dados. \n",
    "\n",
    "Farei um \"undersample\" aleatório das classes majoritárias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "xtW5vPylnWVo",
    "outputId": "8257dba7-f357-4277-88d0-cd86bd010d02",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separo por classes\n",
    "feels = np.array(df['Sentimento'])\n",
    "feelings = [0, 0, 0, 0, 0]\n",
    "for f in feels:\n",
    "    feelings[f] += 1\n",
    "\n",
    "df_class_0 = df[df['Sentimento'] == 0]\n",
    "df_class_1 = df[df['Sentimento'] == 1]\n",
    "df_class_2 = df[df['Sentimento'] == 2]\n",
    "df_class_3 = df[df['Sentimento'] == 3]\n",
    "df_class_4 = df[df['Sentimento'] == 4]\n",
    "\n",
    "# Faço o undersample das classes majoritárias\n",
    "df_class_1 = df_class_1.sample(feelings[0])\n",
    "df_class_2 = df_class_2.sample(feelings[0])\n",
    "df_class_3 = df_class_3.sample(feelings[0])\n",
    "df_class_4 = df_class_4.sample(feelings[0])\n",
    "\n",
    "# Crio um novo DataFrame para classes balanceadas\n",
    "balanced = pd.concat([df_class_0, df_class_1, df_class_2, df_class_3, df_class_4], axis=0)\n",
    "balanced = balanced.sample(frac=1).reset_index(drop=True)\n",
    "feels = np.array(balanced['Sentimento'])\n",
    "feelings = [0, 0, 0, 0, 0]\n",
    "for f in feels:\n",
    "    feelings[f] += 1\n",
    "plt.figure(num=3, figsize=(9, 3))\n",
    "plt.bar(['negativo', 'um pouco negativo', 'neutro',\n",
    "         'um pouco positivo', 'positivo'], feelings)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir daqui, irei apenas repetir o experimento anterior com os três classificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZzO-16E0V1I"
   },
   "outputs": [],
   "source": [
    "X_counts = count_vect.fit_transform(balanced.Texto)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "X_train = X_tfidf[:-1000]\n",
    "X_test = X_tfidf[-1000:]\n",
    "Y_train = balanced.Sentimento[:-1000]\n",
    "Y_test = balanced.Sentimento[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3YILyszlUtP2",
    "outputId": "5c483584-9de9-48ca-a527-b08bfcd8a514",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wLLnxZ_NUtP6",
    "outputId": "3b6802c4-346b-41f1-f5a0-7f5ac1bfe0a8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Multinomial Naive Bayes 47.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Acurácia Multinomial Naive Bayes\",MNB.score(X_test, Y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "aGdcyeXTnlNL",
    "outputId": "41a57cb6-2fdd-469e-ba93-a46c1b48408e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC = LinearSVC()\n",
    "SVC.fit(X_train,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dJ-jCM3epnYS",
    "outputId": "3da5aa5f-5af5-49e3-ab1e-0e8b6fae0ca0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Linear Support Vector Classification 55.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Acurácia Linear Support Vector Classification\",SVC.score(X_test, Y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUrRgtrLUtP_"
   },
   "outputs": [],
   "source": [
    "MLP = Sequential()\n",
    "MLP.add(Dense(units=32, activation='relu', input_dim=X_train.shape[1]))\n",
    "MLP.add(Dropout(0.15))\n",
    "MLP.add(Dense(units=64, activation='relu'))\n",
    "MLP.add(Dropout(0.15))\n",
    "MLP.add(Dense(units=128, activation='relu'))\n",
    "MLP.add(Dropout(0.15))\n",
    "MLP.add(Dense(units=5, activation='softmax'))\n",
    "MLP.compile(loss='categorical_crossentropy', optimizer='sgd',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "APA95m6fGM8Q",
    "outputId": "73dd3112-d6d5-411b-ce41-af72bc3a377a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "34360/34360 [==============================] - 3s 77us/step - loss: 1.6095 - acc: 0.1990\n",
      "Epoch 2/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6094 - acc: 0.2047\n",
      "Epoch 3/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.6093 - acc: 0.2122\n",
      "Epoch 4/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6092 - acc: 0.2124\n",
      "Epoch 5/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6090 - acc: 0.2212\n",
      "Epoch 6/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.6089 - acc: 0.2208\n",
      "Epoch 7/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.6087 - acc: 0.2212\n",
      "Epoch 8/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6085 - acc: 0.2310\n",
      "Epoch 9/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6081 - acc: 0.2398\n",
      "Epoch 10/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6079 - acc: 0.2354\n",
      "Epoch 11/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6075 - acc: 0.2408\n",
      "Epoch 12/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6072 - acc: 0.2499\n",
      "Epoch 13/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6067 - acc: 0.2573\n",
      "Epoch 14/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6063 - acc: 0.2565\n",
      "Epoch 15/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6058 - acc: 0.2650\n",
      "Epoch 16/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6051 - acc: 0.2719\n",
      "Epoch 17/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6045 - acc: 0.2738\n",
      "Epoch 18/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6037 - acc: 0.2800\n",
      "Epoch 19/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6029 - acc: 0.2896\n",
      "Epoch 20/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6018 - acc: 0.2857\n",
      "Epoch 21/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.6006 - acc: 0.2910\n",
      "Epoch 22/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.5994 - acc: 0.3000\n",
      "Epoch 23/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.5980 - acc: 0.3058\n",
      "Epoch 24/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.5960 - acc: 0.3095\n",
      "Epoch 25/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.5938 - acc: 0.3160\n",
      "Epoch 26/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.5917 - acc: 0.3150\n",
      "Epoch 27/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.5886 - acc: 0.3265\n",
      "Epoch 28/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.5855 - acc: 0.3275\n",
      "Epoch 29/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.5812 - acc: 0.3311\n",
      "Epoch 30/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.5768 - acc: 0.3353\n",
      "Epoch 31/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.5712 - acc: 0.3342\n",
      "Epoch 32/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.5641 - acc: 0.3436: 1s - \n",
      "Epoch 33/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.5560 - acc: 0.3439\n",
      "Epoch 34/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.5457 - acc: 0.3476\n",
      "Epoch 35/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.5342 - acc: 0.3562\n",
      "Epoch 36/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.5203 - acc: 0.3644\n",
      "Epoch 37/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.5045 - acc: 0.3635\n",
      "Epoch 38/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.4862 - acc: 0.3735\n",
      "Epoch 39/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.4674 - acc: 0.3814\n",
      "Epoch 40/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.4453 - acc: 0.3894\n",
      "Epoch 41/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.4228 - acc: 0.3992\n",
      "Epoch 42/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.3997 - acc: 0.4058: 0s - loss: 1.4035 -\n",
      "Epoch 43/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 1.3768 - acc: 0.4145\n",
      "Epoch 44/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.3513 - acc: 0.4279: 0s - loss: 1.3509 -\n",
      "Epoch 45/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.3286 - acc: 0.4404\n",
      "Epoch 46/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.3036 - acc: 0.4489\n",
      "Epoch 47/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.2810 - acc: 0.4563\n",
      "Epoch 48/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.2592 - acc: 0.4655\n",
      "Epoch 49/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 1.2369 - acc: 0.4751\n",
      "Epoch 50/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.2188 - acc: 0.4758\n",
      "Epoch 51/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 1.1986 - acc: 0.4851\n",
      "Epoch 52/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.1828 - acc: 0.4921\n",
      "Epoch 53/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.1655 - acc: 0.4970\n",
      "Epoch 54/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.1506 - acc: 0.5056\n",
      "Epoch 55/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 1.1334 - acc: 0.5093\n",
      "Epoch 56/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.1192 - acc: 0.5120\n",
      "Epoch 57/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 1.1079 - acc: 0.5201\n",
      "Epoch 58/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.0932 - acc: 0.5276\n",
      "Epoch 59/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.0828 - acc: 0.5271\n",
      "Epoch 60/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.0776 - acc: 0.5331\n",
      "Epoch 61/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 1.0630 - acc: 0.5389\n",
      "Epoch 62/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.0522 - acc: 0.5397\n",
      "Epoch 63/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.0559 - acc: 0.5379\n",
      "Epoch 64/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.0362 - acc: 0.5504\n",
      "Epoch 65/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.0287 - acc: 0.5517\n",
      "Epoch 66/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.0246 - acc: 0.5494: 1s - l\n",
      "Epoch 67/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.0166 - acc: 0.5581\n",
      "Epoch 68/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.0133 - acc: 0.5594\n",
      "Epoch 69/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 1.0024 - acc: 0.5640\n",
      "Epoch 70/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.9962 - acc: 0.5663\n",
      "Epoch 71/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.9861 - acc: 0.5718\n",
      "Epoch 72/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.9793 - acc: 0.5766\n",
      "Epoch 73/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.9715 - acc: 0.5765\n",
      "Epoch 74/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.9680 - acc: 0.5773\n",
      "Epoch 75/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.9479 - acc: 0.5909\n",
      "Epoch 76/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.9575 - acc: 0.5851\n",
      "Epoch 77/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.9364 - acc: 0.5982\n",
      "Epoch 78/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.9299 - acc: 0.5981\n",
      "Epoch 79/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.9250 - acc: 0.5981\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.9118 - acc: 0.6077\n",
      "Epoch 81/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.9004 - acc: 0.6109\n",
      "Epoch 82/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.9018 - acc: 0.6111\n",
      "Epoch 83/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.9040 - acc: 0.6109\n",
      "Epoch 84/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.8961 - acc: 0.6115\n",
      "Epoch 85/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.8758 - acc: 0.6263\n",
      "Epoch 86/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.8729 - acc: 0.6248\n",
      "Epoch 87/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.8730 - acc: 0.6237\n",
      "Epoch 88/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.8605 - acc: 0.6301\n",
      "Epoch 89/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.8634 - acc: 0.6310\n",
      "Epoch 90/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.8442 - acc: 0.6377\n",
      "Epoch 91/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.8414 - acc: 0.6398\n",
      "Epoch 92/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.8344 - acc: 0.6435\n",
      "Epoch 93/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.8313 - acc: 0.6456\n",
      "Epoch 94/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.8278 - acc: 0.6467\n",
      "Epoch 95/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.8282 - acc: 0.6437\n",
      "Epoch 96/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.8099 - acc: 0.6519\n",
      "Epoch 97/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.8182 - acc: 0.6528\n",
      "Epoch 98/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7979 - acc: 0.6601\n",
      "Epoch 99/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.8056 - acc: 0.6582\n",
      "Epoch 100/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7955 - acc: 0.6616\n",
      "Epoch 101/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7876 - acc: 0.6656\n",
      "Epoch 102/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7853 - acc: 0.6666\n",
      "Epoch 103/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7713 - acc: 0.6713\n",
      "Epoch 104/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.7659 - acc: 0.6742\n",
      "Epoch 105/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.7797 - acc: 0.6695\n",
      "Epoch 106/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7519 - acc: 0.6811\n",
      "Epoch 107/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.7582 - acc: 0.6780\n",
      "Epoch 108/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7386 - acc: 0.6881\n",
      "Epoch 109/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7513 - acc: 0.6832\n",
      "Epoch 110/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.7395 - acc: 0.6866\n",
      "Epoch 111/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7438 - acc: 0.6847\n",
      "Epoch 112/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7535 - acc: 0.6855\n",
      "Epoch 113/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7314 - acc: 0.6923\n",
      "Epoch 114/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7257 - acc: 0.6935\n",
      "Epoch 115/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7201 - acc: 0.6974\n",
      "Epoch 116/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.7118 - acc: 0.7011\n",
      "Epoch 117/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7090 - acc: 0.7010\n",
      "Epoch 118/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.7068 - acc: 0.7039\n",
      "Epoch 119/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.7056 - acc: 0.7034\n",
      "Epoch 120/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.6965 - acc: 0.7039\n",
      "Epoch 121/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6901 - acc: 0.7102\n",
      "Epoch 122/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.6991 - acc: 0.7084\n",
      "Epoch 123/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6862 - acc: 0.7146\n",
      "Epoch 124/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6913 - acc: 0.7088\n",
      "Epoch 125/500\n",
      "34360/34360 [==============================] - 2s 68us/step - loss: 0.6859 - acc: 0.7132\n",
      "Epoch 126/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.6802 - acc: 0.7169\n",
      "Epoch 127/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6597 - acc: 0.7271\n",
      "Epoch 128/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.6635 - acc: 0.7276\n",
      "Epoch 129/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.6647 - acc: 0.7233\n",
      "Epoch 130/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6703 - acc: 0.7208\n",
      "Epoch 131/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6482 - acc: 0.7286\n",
      "Epoch 132/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6565 - acc: 0.7273\n",
      "Epoch 133/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6504 - acc: 0.7294\n",
      "Epoch 134/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6464 - acc: 0.7350\n",
      "Epoch 135/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.6619 - acc: 0.7235\n",
      "Epoch 136/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6388 - acc: 0.7365\n",
      "Epoch 137/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6522 - acc: 0.7291\n",
      "Epoch 138/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6314 - acc: 0.7404\n",
      "Epoch 139/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6326 - acc: 0.7373\n",
      "Epoch 140/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6366 - acc: 0.7345\n",
      "Epoch 141/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.6274 - acc: 0.7405\n",
      "Epoch 142/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6369 - acc: 0.7362\n",
      "Epoch 143/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6104 - acc: 0.7505\n",
      "Epoch 144/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.6167 - acc: 0.7451\n",
      "Epoch 145/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6196 - acc: 0.7435\n",
      "Epoch 146/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5971 - acc: 0.7549\n",
      "Epoch 147/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6127 - acc: 0.7512\n",
      "Epoch 148/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5980 - acc: 0.7531\n",
      "Epoch 149/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6219 - acc: 0.7466\n",
      "Epoch 150/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.6118 - acc: 0.7481\n",
      "Epoch 151/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5848 - acc: 0.7614\n",
      "Epoch 152/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5860 - acc: 0.7639\n",
      "Epoch 153/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5889 - acc: 0.7634\n",
      "Epoch 154/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5669 - acc: 0.7712\n",
      "Epoch 155/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5885 - acc: 0.7620\n",
      "Epoch 156/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5812 - acc: 0.7638\n",
      "Epoch 157/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5698 - acc: 0.7666\n",
      "Epoch 158/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5750 - acc: 0.7650\n",
      "Epoch 159/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5756 - acc: 0.7662\n",
      "Epoch 160/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5599 - acc: 0.7705\n",
      "Epoch 161/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5465 - acc: 0.7777\n",
      "Epoch 162/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5603 - acc: 0.7739\n",
      "Epoch 163/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5605 - acc: 0.7728\n",
      "Epoch 164/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5552 - acc: 0.7756\n",
      "Epoch 165/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5953 - acc: 0.7579\n",
      "Epoch 166/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.6219 - acc: 0.7451\n",
      "Epoch 167/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5666 - acc: 0.7680\n",
      "Epoch 168/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5223 - acc: 0.7913\n",
      "Epoch 169/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5295 - acc: 0.7856\n",
      "Epoch 170/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.5203 - acc: 0.7885\n",
      "Epoch 171/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5317 - acc: 0.7867\n",
      "Epoch 172/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5289 - acc: 0.7879\n",
      "Epoch 173/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5426 - acc: 0.7839\n",
      "Epoch 174/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.6005 - acc: 0.7571\n",
      "Epoch 175/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5517 - acc: 0.7763\n",
      "Epoch 176/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5933 - acc: 0.7631\n",
      "Epoch 177/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5298 - acc: 0.7859\n",
      "Epoch 178/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5578 - acc: 0.7798\n",
      "Epoch 179/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.5268 - acc: 0.7876\n",
      "Epoch 180/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.5110 - acc: 0.7925\n",
      "Epoch 181/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5200 - acc: 0.7913\n",
      "Epoch 182/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.5310 - acc: 0.7859\n",
      "Epoch 183/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5189 - acc: 0.7872\n",
      "Epoch 184/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4816 - acc: 0.8089\n",
      "Epoch 185/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.4969 - acc: 0.7999\n",
      "Epoch 186/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4833 - acc: 0.8071\n",
      "Epoch 187/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.4960 - acc: 0.8040\n",
      "Epoch 188/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4747 - acc: 0.8112\n",
      "Epoch 189/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4774 - acc: 0.8099\n",
      "Epoch 190/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4925 - acc: 0.8042\n",
      "Epoch 191/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4883 - acc: 0.8047\n",
      "Epoch 192/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4782 - acc: 0.8083\n",
      "Epoch 193/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4692 - acc: 0.8118\n",
      "Epoch 194/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4686 - acc: 0.8147\n",
      "Epoch 195/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4572 - acc: 0.8166\n",
      "Epoch 196/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5027 - acc: 0.7969\n",
      "Epoch 197/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4877 - acc: 0.8079\n",
      "Epoch 198/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.5215 - acc: 0.7949\n",
      "Epoch 199/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4920 - acc: 0.8036\n",
      "Epoch 200/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.5060 - acc: 0.7951\n",
      "Epoch 201/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4882 - acc: 0.8042\n",
      "Epoch 202/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4711 - acc: 0.8124\n",
      "Epoch 203/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4339 - acc: 0.8309\n",
      "Epoch 204/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.4770 - acc: 0.8157\n",
      "Epoch 205/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4358 - acc: 0.8294\n",
      "Epoch 206/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4385 - acc: 0.8269\n",
      "Epoch 207/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4766 - acc: 0.8137\n",
      "Epoch 208/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4463 - acc: 0.8249\n",
      "Epoch 209/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4535 - acc: 0.8240\n",
      "Epoch 210/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.4364 - acc: 0.8287\n",
      "Epoch 211/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4486 - acc: 0.8228\n",
      "Epoch 212/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4951 - acc: 0.8092\n",
      "Epoch 213/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.5504 - acc: 0.7755\n",
      "Epoch 214/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4889 - acc: 0.8095\n",
      "Epoch 215/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5312 - acc: 0.7915\n",
      "Epoch 216/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4657 - acc: 0.8130\n",
      "Epoch 217/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4992 - acc: 0.8044\n",
      "Epoch 218/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4469 - acc: 0.8228\n",
      "Epoch 219/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.5248 - acc: 0.7946\n",
      "Epoch 220/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4528 - acc: 0.8207\n",
      "Epoch 221/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.4405 - acc: 0.8258\n",
      "Epoch 222/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.4366 - acc: 0.8263\n",
      "Epoch 223/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4478 - acc: 0.8203\n",
      "Epoch 224/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4313 - acc: 0.8315\n",
      "Epoch 225/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.4354 - acc: 0.8260\n",
      "Epoch 226/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4278 - acc: 0.8289\n",
      "Epoch 227/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.4278 - acc: 0.8350\n",
      "Epoch 228/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4530 - acc: 0.8236\n",
      "Epoch 229/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.4091 - acc: 0.8370\n",
      "Epoch 230/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4610 - acc: 0.8208\n",
      "Epoch 231/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.4385 - acc: 0.8294\n",
      "Epoch 232/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3923 - acc: 0.8467\n",
      "Epoch 233/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4344 - acc: 0.8323\n",
      "Epoch 234/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4492 - acc: 0.8166\n",
      "Epoch 235/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4439 - acc: 0.8224\n",
      "Epoch 236/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4627 - acc: 0.8159\n",
      "Epoch 237/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4060 - acc: 0.8409\n",
      "Epoch 238/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3843 - acc: 0.8492\n",
      "Epoch 239/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4628 - acc: 0.8244\n",
      "Epoch 240/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.4794 - acc: 0.8109\n",
      "Epoch 241/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5401 - acc: 0.7869\n",
      "Epoch 242/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4427 - acc: 0.8237\n",
      "Epoch 243/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4735 - acc: 0.8199\n",
      "Epoch 244/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4021 - acc: 0.8416\n",
      "Epoch 245/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3875 - acc: 0.8500\n",
      "Epoch 246/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3963 - acc: 0.8446\n",
      "Epoch 247/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3933 - acc: 0.8463\n",
      "Epoch 248/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3734 - acc: 0.8554\n",
      "Epoch 249/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4352 - acc: 0.8313\n",
      "Epoch 250/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4292 - acc: 0.8344\n",
      "Epoch 251/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3750 - acc: 0.8563\n",
      "Epoch 252/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3806 - acc: 0.8522\n",
      "Epoch 253/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3915 - acc: 0.8484\n",
      "Epoch 254/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.4358 - acc: 0.8372\n",
      "Epoch 255/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3940 - acc: 0.8476\n",
      "Epoch 256/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.4514 - acc: 0.8275\n",
      "Epoch 257/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5008 - acc: 0.8018\n",
      "Epoch 258/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.5162 - acc: 0.8004\n",
      "Epoch 259/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4723 - acc: 0.8150\n",
      "Epoch 260/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4667 - acc: 0.8204\n",
      "Epoch 261/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4440 - acc: 0.8209\n",
      "Epoch 262/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.4128 - acc: 0.8374\n",
      "Epoch 263/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3866 - acc: 0.8461\n",
      "Epoch 264/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.3737 - acc: 0.8536\n",
      "Epoch 265/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3597 - acc: 0.8568\n",
      "Epoch 266/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.3609 - acc: 0.8580\n",
      "Epoch 267/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.4009 - acc: 0.8434\n",
      "Epoch 268/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3666 - acc: 0.8583\n",
      "Epoch 269/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3636 - acc: 0.8576\n",
      "Epoch 270/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3602 - acc: 0.8607\n",
      "Epoch 271/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3608 - acc: 0.8599\n",
      "Epoch 272/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3816 - acc: 0.8504\n",
      "Epoch 273/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3480 - acc: 0.8662\n",
      "Epoch 274/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3866 - acc: 0.8481\n",
      "Epoch 275/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3864 - acc: 0.8495: 0s - loss: 0.3821 - acc: 0.8\n",
      "Epoch 276/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3724 - acc: 0.8548\n",
      "Epoch 277/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3405 - acc: 0.8670\n",
      "Epoch 278/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3482 - acc: 0.8612\n",
      "Epoch 279/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.3522 - acc: 0.8620\n",
      "Epoch 280/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3552 - acc: 0.8611\n",
      "Epoch 281/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3391 - acc: 0.8692\n",
      "Epoch 282/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3417 - acc: 0.8661\n",
      "Epoch 283/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.3587 - acc: 0.8613\n",
      "Epoch 284/500\n",
      "34360/34360 [==============================] - 2s 68us/step - loss: 0.3338 - acc: 0.8730\n",
      "Epoch 285/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3454 - acc: 0.8664\n",
      "Epoch 286/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.3331 - acc: 0.8718\n",
      "Epoch 287/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3309 - acc: 0.8744\n",
      "Epoch 288/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3372 - acc: 0.8682\n",
      "Epoch 289/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3355 - acc: 0.8687\n",
      "Epoch 290/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3340 - acc: 0.8690: 1s - \n",
      "Epoch 291/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3735 - acc: 0.8591\n",
      "Epoch 292/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3353 - acc: 0.8696\n",
      "Epoch 293/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3256 - acc: 0.8748\n",
      "Epoch 294/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3183 - acc: 0.8761\n",
      "Epoch 295/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3678 - acc: 0.8607\n",
      "Epoch 296/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3188 - acc: 0.8756\n",
      "Epoch 297/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.3374 - acc: 0.8733\n",
      "Epoch 298/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3232 - acc: 0.8747\n",
      "Epoch 299/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3148 - acc: 0.8774\n",
      "Epoch 300/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.3173 - acc: 0.8773\n",
      "Epoch 301/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3205 - acc: 0.8744\n",
      "Epoch 302/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.4489 - acc: 0.8357\n",
      "Epoch 303/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3249 - acc: 0.8741\n",
      "Epoch 304/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3268 - acc: 0.8722\n",
      "Epoch 305/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3171 - acc: 0.8782\n",
      "Epoch 306/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.3306 - acc: 0.8722\n",
      "Epoch 307/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.3118 - acc: 0.8774\n",
      "Epoch 308/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2952 - acc: 0.8859: \n",
      "Epoch 309/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2986 - acc: 0.8837\n",
      "Epoch 310/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3071 - acc: 0.8804\n",
      "Epoch 311/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3141 - acc: 0.8752\n",
      "Epoch 312/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2928 - acc: 0.8858\n",
      "Epoch 313/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2996 - acc: 0.8839\n",
      "Epoch 314/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2924 - acc: 0.8862\n",
      "Epoch 315/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3082 - acc: 0.8801\n",
      "Epoch 316/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.3208 - acc: 0.8753\n",
      "Epoch 317/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3124 - acc: 0.8802\n",
      "Epoch 318/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2918 - acc: 0.8865\n",
      "Epoch 319/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2895 - acc: 0.8880\n",
      "Epoch 320/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2986 - acc: 0.8861\n",
      "Epoch 321/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2845 - acc: 0.8898\n",
      "Epoch 322/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2746 - acc: 0.8941\n",
      "Epoch 323/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2890 - acc: 0.8867\n",
      "Epoch 324/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3042 - acc: 0.8841\n",
      "Epoch 325/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2784 - acc: 0.8944\n",
      "Epoch 326/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2727 - acc: 0.8940\n",
      "Epoch 327/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2803 - acc: 0.8922\n",
      "Epoch 328/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2890 - acc: 0.8882\n",
      "Epoch 329/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2752 - acc: 0.8923\n",
      "Epoch 330/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2943 - acc: 0.8843\n",
      "Epoch 331/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.3027 - acc: 0.8829\n",
      "Epoch 332/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2757 - acc: 0.8934\n",
      "Epoch 333/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2751 - acc: 0.8934\n",
      "Epoch 334/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2705 - acc: 0.8962\n",
      "Epoch 335/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.2736 - acc: 0.8925\n",
      "Epoch 336/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2687 - acc: 0.8968\n",
      "Epoch 337/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.2693 - acc: 0.8953\n",
      "Epoch 338/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2923 - acc: 0.8894\n",
      "Epoch 339/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2656 - acc: 0.8975\n",
      "Epoch 340/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.2568 - acc: 0.9017\n",
      "Epoch 341/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2621 - acc: 0.8998\n",
      "Epoch 342/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2632 - acc: 0.8980\n",
      "Epoch 343/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2582 - acc: 0.9006\n",
      "Epoch 344/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2575 - acc: 0.9003\n",
      "Epoch 345/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2518 - acc: 0.9012\n",
      "Epoch 346/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2621 - acc: 0.8978\n",
      "Epoch 347/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2525 - acc: 0.9029\n",
      "Epoch 348/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2924 - acc: 0.8893\n",
      "Epoch 349/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2463 - acc: 0.9054\n",
      "Epoch 350/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2546 - acc: 0.9008\n",
      "Epoch 351/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2534 - acc: 0.9011\n",
      "Epoch 352/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2492 - acc: 0.9036\n",
      "Epoch 353/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2509 - acc: 0.9020\n",
      "Epoch 354/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2607 - acc: 0.9004\n",
      "Epoch 355/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2519 - acc: 0.9031\n",
      "Epoch 356/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2471 - acc: 0.9041\n",
      "Epoch 357/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2466 - acc: 0.9027\n",
      "Epoch 358/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2394 - acc: 0.9076: 0s - loss: 0.2\n",
      "Epoch 359/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2449 - acc: 0.9061\n",
      "Epoch 360/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2397 - acc: 0.9070\n",
      "Epoch 361/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2392 - acc: 0.9067\n",
      "Epoch 362/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2433 - acc: 0.9056\n",
      "Epoch 363/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2463 - acc: 0.9035\n",
      "Epoch 364/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2677 - acc: 0.8998\n",
      "Epoch 365/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2946 - acc: 0.8894: 0s - loss: 0.\n",
      "Epoch 366/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2468 - acc: 0.9047\n",
      "Epoch 367/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2580 - acc: 0.8987\n",
      "Epoch 368/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2748 - acc: 0.8931\n",
      "Epoch 369/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2537 - acc: 0.9025\n",
      "Epoch 370/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2570 - acc: 0.9001\n",
      "Epoch 371/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2275 - acc: 0.9124\n",
      "Epoch 372/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2334 - acc: 0.9091\n",
      "Epoch 373/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2336 - acc: 0.9098\n",
      "Epoch 374/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2337 - acc: 0.9092\n",
      "Epoch 375/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2323 - acc: 0.9086\n",
      "Epoch 376/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2380 - acc: 0.9068\n",
      "Epoch 377/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2384 - acc: 0.9078\n",
      "Epoch 378/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2339 - acc: 0.9094\n",
      "Epoch 379/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2320 - acc: 0.9090\n",
      "Epoch 380/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2319 - acc: 0.9090\n",
      "Epoch 381/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2301 - acc: 0.9108\n",
      "Epoch 382/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2261 - acc: 0.9128\n",
      "Epoch 383/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2263 - acc: 0.9130\n",
      "Epoch 384/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2293 - acc: 0.9105\n",
      "Epoch 385/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2260 - acc: 0.9120\n",
      "Epoch 386/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2462 - acc: 0.9055\n",
      "Epoch 387/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2241 - acc: 0.9130\n",
      "Epoch 388/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2238 - acc: 0.9141\n",
      "Epoch 389/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2255 - acc: 0.9127\n",
      "Epoch 390/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2335 - acc: 0.9089\n",
      "Epoch 391/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2254 - acc: 0.9135\n",
      "Epoch 392/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2201 - acc: 0.9153\n",
      "Epoch 393/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2250 - acc: 0.9118\n",
      "Epoch 394/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2168 - acc: 0.9162\n",
      "Epoch 395/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2293 - acc: 0.9102\n",
      "Epoch 396/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2138 - acc: 0.9161\n",
      "Epoch 397/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2143 - acc: 0.9156\n",
      "Epoch 398/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2310 - acc: 0.9119\n",
      "Epoch 399/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2139 - acc: 0.9160\n",
      "Epoch 400/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2188 - acc: 0.9135\n",
      "Epoch 401/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2090 - acc: 0.9182\n",
      "Epoch 402/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2148 - acc: 0.9170\n",
      "Epoch 403/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2111 - acc: 0.9180\n",
      "Epoch 404/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2155 - acc: 0.9162\n",
      "Epoch 405/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2151 - acc: 0.9165\n",
      "Epoch 406/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2144 - acc: 0.9169\n",
      "Epoch 407/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.2083 - acc: 0.9182\n",
      "Epoch 408/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2087 - acc: 0.9188\n",
      "Epoch 409/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2121 - acc: 0.9178\n",
      "Epoch 410/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.2098 - acc: 0.9191\n",
      "Epoch 411/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2268 - acc: 0.9107\n",
      "Epoch 412/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2091 - acc: 0.9159\n",
      "Epoch 413/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2150 - acc: 0.9161\n",
      "Epoch 414/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2163 - acc: 0.9148\n",
      "Epoch 415/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2028 - acc: 0.9208\n",
      "Epoch 416/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2091 - acc: 0.9195\n",
      "Epoch 417/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2159 - acc: 0.9152\n",
      "Epoch 418/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2032 - acc: 0.9221\n",
      "Epoch 419/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2040 - acc: 0.9193\n",
      "Epoch 420/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.2016 - acc: 0.9212\n",
      "Epoch 421/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.2009 - acc: 0.9233\n",
      "Epoch 422/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2023 - acc: 0.9212\n",
      "Epoch 423/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2056 - acc: 0.9181\n",
      "Epoch 424/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2047 - acc: 0.9207\n",
      "Epoch 425/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2075 - acc: 0.9183\n",
      "Epoch 426/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1973 - acc: 0.9234\n",
      "Epoch 427/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2004 - acc: 0.9225\n",
      "Epoch 428/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.2497 - acc: 0.9064\n",
      "Epoch 429/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2014 - acc: 0.9202\n",
      "Epoch 430/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.1992 - acc: 0.9235\n",
      "Epoch 431/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2084 - acc: 0.9180\n",
      "Epoch 432/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2372 - acc: 0.9104\n",
      "Epoch 433/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1940 - acc: 0.9252\n",
      "Epoch 434/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2006 - acc: 0.9204\n",
      "Epoch 435/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2028 - acc: 0.9197\n",
      "Epoch 436/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1997 - acc: 0.9227\n",
      "Epoch 437/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2012 - acc: 0.9214\n",
      "Epoch 438/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1920 - acc: 0.9237\n",
      "Epoch 439/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2034 - acc: 0.9207\n",
      "Epoch 440/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1932 - acc: 0.9247\n",
      "Epoch 441/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2021 - acc: 0.9205\n",
      "Epoch 442/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1948 - acc: 0.9240\n",
      "Epoch 443/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1911 - acc: 0.9235\n",
      "Epoch 444/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1964 - acc: 0.9238\n",
      "Epoch 445/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.3029 - acc: 0.8950\n",
      "Epoch 446/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2454 - acc: 0.9064\n",
      "Epoch 447/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2093 - acc: 0.9171\n",
      "Epoch 448/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2006 - acc: 0.9224\n",
      "Epoch 449/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1984 - acc: 0.9218\n",
      "Epoch 450/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2005 - acc: 0.9225\n",
      "Epoch 451/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.1982 - acc: 0.9227\n",
      "Epoch 452/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2151 - acc: 0.9164\n",
      "Epoch 453/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2075 - acc: 0.9197\n",
      "Epoch 454/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.1921 - acc: 0.9239\n",
      "Epoch 455/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1921 - acc: 0.9249\n",
      "Epoch 456/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1896 - acc: 0.9256\n",
      "Epoch 457/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1958 - acc: 0.9238\n",
      "Epoch 458/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1931 - acc: 0.9254\n",
      "Epoch 459/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1942 - acc: 0.9225\n",
      "Epoch 460/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.1934 - acc: 0.9247\n",
      "Epoch 461/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.1908 - acc: 0.9250\n",
      "Epoch 462/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.2430 - acc: 0.9076\n",
      "Epoch 463/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1966 - acc: 0.9234\n",
      "Epoch 464/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1942 - acc: 0.9230\n",
      "Epoch 465/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.1972 - acc: 0.9214\n",
      "Epoch 466/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.2050 - acc: 0.9205\n",
      "Epoch 467/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.1971 - acc: 0.9213\n",
      "Epoch 468/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.1896 - acc: 0.9258\n",
      "Epoch 469/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1873 - acc: 0.9263\n",
      "Epoch 470/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1881 - acc: 0.9245\n",
      "Epoch 471/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.1937 - acc: 0.9242\n",
      "Epoch 472/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.1857 - acc: 0.9268\n",
      "Epoch 473/500\n",
      "34360/34360 [==============================] - 2s 67us/step - loss: 0.1869 - acc: 0.9265\n",
      "Epoch 474/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1899 - acc: 0.9254\n",
      "Epoch 475/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1899 - acc: 0.9242\n",
      "Epoch 476/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1949 - acc: 0.9235\n",
      "Epoch 477/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1830 - acc: 0.9278\n",
      "Epoch 478/500\n",
      "34360/34360 [==============================] - 2s 66us/step - loss: 0.1859 - acc: 0.9252\n",
      "Epoch 479/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1864 - acc: 0.9253\n",
      "Epoch 480/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.2273 - acc: 0.9102\n",
      "Epoch 481/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1894 - acc: 0.9262\n",
      "Epoch 482/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1872 - acc: 0.9259\n",
      "Epoch 483/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1907 - acc: 0.9244\n",
      "Epoch 484/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.1841 - acc: 0.9272\n",
      "Epoch 485/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1843 - acc: 0.9275\n",
      "Epoch 486/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1767 - acc: 0.9307\n",
      "Epoch 487/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1739 - acc: 0.9306\n",
      "Epoch 488/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1787 - acc: 0.9297\n",
      "Epoch 489/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1806 - acc: 0.9283\n",
      "Epoch 490/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1801 - acc: 0.9287\n",
      "Epoch 491/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.1797 - acc: 0.9288\n",
      "Epoch 492/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.1725 - acc: 0.9307\n",
      "Epoch 493/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1775 - acc: 0.9291\n",
      "Epoch 494/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1764 - acc: 0.9286\n",
      "Epoch 495/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.1700 - acc: 0.9335\n",
      "Epoch 496/500\n",
      "34360/34360 [==============================] - 2s 64us/step - loss: 0.1736 - acc: 0.9310\n",
      "Epoch 497/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1684 - acc: 0.9334\n",
      "Epoch 498/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1776 - acc: 0.9304\n",
      "Epoch 499/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1708 - acc: 0.9333\n",
      "Epoch 500/500\n",
      "34360/34360 [==============================] - 2s 65us/step - loss: 0.1705 - acc: 0.9304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2ce98de80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = []\n",
    "for label in balanced.Sentimento[:-1000]:\n",
    "    l = [0 for _ in range(5)]\n",
    "    l[label] = 1\n",
    "    Y.append(l)\n",
    "MLP.fit(X_train, np.array(Y), epochs=500, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fglYB_8oUtQH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 147us/step\n",
      "Acurácia Multilayer Perceptron 57.60000023841858\n"
     ]
    }
   ],
   "source": [
    "y_test = []\n",
    "for label in Y_test:\n",
    "    l = [0 for _ in range(5)]\n",
    "    l[label] = 1\n",
    "    y_test.append(l)\n",
    "loss_and_metrics = MLP.evaluate(X_test, np.array(y_test), batch_size=128)\n",
    "print(\"Acurácia Multilayer Perceptron\",loss_and_metrics[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "\n",
    "Obtivemos resultados interessantes.\n",
    "Como esperado, com o Naive Bayes com o balanceamento tivemos uma acurácia menor, pois quanto mais dados, melhor para o algoritmo, independendo do balanceamento dos dados.\n",
    "\n",
    "Os algoritmos de LinearSVC e MLP me surpreenderam após o balanceamento dos dados. Talvez pela imensa perda de dados que tivemos quando fiz o undersample, eles tiveram uma perfomance menor do que com os dados desbalanceados. Talvez com um oversample ou um tipo de geração de protótipos (ex. LVQ3), poderíamos ter um resultado mais eficiente.\n",
    "\n",
    "O algoritmo que se sobressiu foi o de LinearSVC com 61% de acurácia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que gostaria ter incluído no projeto\n",
    "\n",
    "Como me interesso muito pela área de Deep Learning, gostaria de ter aplicado alguma das técnicas citadas em \"Deep Learning for Sentiment Analysis : A Survey\", Lei Zhang, Shuai Wang, Bing Liu. Infelizmente, por ser novo na área de análise de texto, não tive o tempo suficiente para pesquisar sobre como aplicar no contexto dado.\n",
    "\n",
    "Um detalhe que queria ter feito é não ter usado o pré processamento do Scikit e ter desenvolvido um próprio. Ainda comecei a desenvolver um mas não daria tempo de criar um bom vetor de palavras utilizando uma lookup table de matrizes esparças, como faz o scikit.\n",
    "\n",
    "Algo que me deixou curioso e não consegui aplicar foi o Word2Vec/Sen2Vec/Doc2Vec. Pesquisei por cima como eram treinados os modelos mas não achei nenhum que retornasse o modelo como um numpy array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agradeço a oportunidade\n",
    "Abraços!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Intelivix.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
